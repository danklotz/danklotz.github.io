<!DOCTYPE html>


<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>media diet | daniel klotz |</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
  </head>

<body>
<center>
  <img src="/danklotz-signature.png", style="height:230px">
  <p style="margin-bottom:-35px;"></p>
  <ul class="menu", margin=0>·
    
    <a href="/">hi</a> ·
    
    <a href="/note/">notes</a> ·
    
    <a href="/gfx/">gfx</a> ·
    
    <a href="/about/">me</a> ·
    
  </ul>
  <br>
 </center>
<p style="margin-bottom:-50px;"></p>
</body>

<div class="article-meta">

<h1><span class="title">media diet</span></h1>


</div>

<main>
<h2 id="juli2020">Juli.2020</h2>
<h3 id="books">Books</h3>
<ul>
<li><strong>Aron Graplin - Draplin Desing Co.: Pretty Much Everything</strong> &hellip; Very nice book. For me, graphic design does not get much better than this.</li>
<li><strong>Stephen Stigler - The Seven Pillars of Statistical Wisdom</strong> &hellip; Pretty good read. Somethimes to simple, sometimes a bit to short of an exposition.</li>
<li><strong>Stephen Stigler - Statistics on the Table: The history of Statistical Concepts and Methods (Revised)</strong> &hellip; This is a great book. Incredibly informative.</li>
<li><strong>Richard Von Mises - Probability, Statistics and Truth</strong> &hellip; Great read. In hindsight it seems that Von Mises underestimated how difficult randomness is.</li>
<li><strong>The Social and Environmental Effects of Large Damns: Volume Two</strong> &hellip; Surprisingly intersting set of case studies that emphasize the negative impacts of large damns.</li>
</ul>
<h3 id="movies">Movies</h3>
<ul>
<li><strong><a href="https://www.imdb.com/title/tt2531336/?ref_=fn_al_tt_1">Lupin Season 2</a></strong> &hellip; Nice and simple french series. I like how they pla yon the books and the performance from <a href="https://www.imdb.com/name/nm1082477/?ref_=tt_cl_t_1">Omar Sy</a> is great.</li>
<li><strong><a href="https://www.imdb.com/title/tt8367814/?ref_=nm_flmg_wr_4">The Gentlemen</a></strong> &hellip; Very fun and samrt movie. Happy to see that <a href="https://www.imdb.com/name/nm0005363/?ref_=tt_ov_wr">Guy Ritchie</a> still makes movies like this.</li>
<li><strong><a href="https://www.imdb.com/title/tt11083552/?ref_=nm_flmg_wr_3">Wrath of Man</a></strong> &hellip; Medicore at best.</li>
<li><strong><a href="https://www.imdb.com/title/tt0087481/?ref_=nv_sr_srsg_7">Non ce due senza quatro</a></strong> &hellip; Still great.</li>
<li><strong>Nati con la Camicia</strong> &hellip; Still great.</li>
</ul>
<h3 id="learn">Learn</h3>
<ul>
<li><a href="https://youtu.be/yGyv-2O4bNU">Jordan Ellenberg discusses &ldquo;Shape&rdquo; with Cathy O’Neil</a> &hellip; nice book sell.</li>
<li><a href="https://youtu.be/OGEgT_qgikY">The Tragic Tale of Notch</a> &hellip; did not follow the story of notch. Pretty sobering.</li>
<li><a href="https://www.youtube.com/channel/UC8uY6yLP9BS4BUc9BSc0Jww">Taleb&rsquo;s Mini Lectures in Probability</a>. <a href="https://twitter.com/nntaleb">Taleb</a> started a new set of mini lessons about probability. They are great!</li>
<li><a href="https://youtu.be/8BbsmzPdkO4">ISF 2021 Keynote PSeaker: Gerd Gigerenzer</a> &hellip; I like Gerds view on things, even (because?) if it does not seem to have changed for a long time.</li>
<li><a href="https://youtu.be/Pgezk2hTe7w">Wendy Parker: Explaining Climate Change: Important Roles for Computer Simulation </a> &hellip; Not sure if I agree with everything. But nice lecture.</li>
<li><a href="https://www.youtube.com/playlist?list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd">Philip Henning (2021): Probabilistic Machine Learning</a> &hellip; Great series of lectures.</li>
<li><a href="https://youtu.be/kUZpZ2-5Ow0">Tim Hunkin</a> &hellip; on the importance of tinkering and repairing.</li>
<li><a href="https://youtu.be/7J52mDjZzto">Heat Pumps: the Future of Home Heating</a> &hellip; Interesting. And just the first of two.</li>
<li><a href="https://youtu.be/6s4gMiiDhOQ">Nancy Cartwright - Responsible Science: Responsible Use</a> &hellip; Very thoughtful lectures by one of the smartest people I am aware of.</li>
<li><a href="https://youtu.be/swWAPqVHqDs">Jean Jullien (2020): Nicer Tuesdays</a> &hellip; his illustrations and paintings always bring a smile to my face &lt;3.</li>
<li><a href="https://youtu.be/vIS948rftEA">Essential Governing Principles of the Biosphere and Ecological Engineering</a> &hellip; more a repetition for me &ndash; but a good one.</li>
</ul>
<h3 id="random-youtubes">Random Youtubes</h3>
<ul>
<li><a href="https://www.youtube.com/c/JoyBoyTheories">Joy Boy Theories</a> &hellip; One Piece Theories and Reviews. Also some other stuff.</li>
<li><a href="https://www.youtube.com/c/howtodrink">How to Drink</a> &hellip; creative cocktail inspiration and history lectures.</li>
<li><a href="https://www.youtube.com/channel/UCcxN_gfAE_43Wj6-UWC16bw">Kyoto Video</a> &hellip; channel about the history of anime and cartoons (mostly anime, mostly 90s)
Very random one. Mostly some brain candy; some more, some less healthy.</li>
<li><a href="https://www.youtube.com/user/RedLetterMedia">RedLetterMedia</a> &hellip; best channel for movie reviews.</li>
<li><a href="https://www.youtube.com/channel/UCAdxbl2h9eTF4F-XSYSud4w">Brian DiMambro - Rare Books &amp; Antique Maps</a> &hellip; The channel is what it says. I don&rsquo;t know why I like it.</li>
</ul>
<hr>
<h2 id="30mai2020">30.Mai.2020</h2>
<ul>
<li><a href="https://www.youtube.com/user/seseragistudio">Dave Bull</a> is a maker of japanes woodblock prints and <a href="https://youtu.be/ij9KXgiyDAc">intersting</a> <a href="https://youtu.be/R9i0Yq89pa0">stories</a> to tell.</li>
<li>The muji <a href="https://www.muji.eu/pages/online.asp?Sec=13&amp;Sub=54&amp;PID=8504">fountain pen</a> works great.</li>
<li>I do not get how audible works. (comment: <em>I do now &amp; I dont like it.</em> )</li>
<li><a href="https://www.youtube.com/channel/UCT2OtOBwEATHu_Ku1mWHCrw">The Book Brood</a> is a strange yet endearing youtube channel about &hellip; books and family life. The [green renaissance] makes captivating videos about individuals (<a href="https://youtu.be/MPWta7ZNFpc">e.g.</a>).</li>
<li><a href="https://www.youtube.com/channel/UCyB-IBVQ_Za8M2A6Atmm1mg">Geometric cakes</a>.</li>
<li><a href="https://youtu.be/6IJqNnRtWaM">Glenn Shafer</a> knows where confidence intervals come from (<a href="https://www.researchers.one/article/2019-09-22">see also</a>, and note that confidence intervals are <a href="https://en.wikipedia.org/wiki/Confidence_and_prediction_bands">not prediciton intervals</a>).</li>
<li>Cédric Villani gives an <a href="https://youtu.be/QO0yixTmqhM">incomplete account</a> of how machine learning came into what it is today.</li>
<li><a href="https://youtu.be/HNrA3dMS8wc">Analog computation</a> is cool (<a href="https://en.wikipedia.org/wiki/Analog_computer">see also</a>). <a href="https://youtu.be/4i2QDSn7uRo">Yes</a>. <a href="https://youtu.be/rKeFCd1j5BE">No</a>.</li>
<li><a href="https://www.youtube.com/watch?v=dGuQHNezwt4">Yes, cats</a>.</li>
<li><a href="https://youtu.be/tn_16d74kfc">This thing</a> is strange; I do not understand what it is. Might be fraud, or he might be on to something. <a href="https://youtu.be/HQl_2yhX3Bg">Cousteau was cool</a>.</li>
<li><a href="https://youtu.be/PxfDwtaYo5I">This one</a> is more about probability than about statistics, but (and actually because of that) interesting.</li>
</ul>
<hr>
<h2 id="31january2020">31.January2020</h2>
<h3 id="movies-1">Movies</h3>
<ul>
<li><strong>Succession S2:</strong> Quite good. Would recommend. 3/5.</li>
<li><strong>Once Upon a Time in Hollywood:</strong> Very long movie, but never got boring. Maybe the most accessible Tarantino movie; especially if you know the background story. 5/5</li>
<li><strong>From up a Poppy Hill:</strong> Super sweet movie from Goro Miyazaki. I did not like his Earthsea movie, but I liked this one a lot. 4/5</li>
</ul>
<h3 id="web">Web</h3>
<ul>
<li><strong><a href="josenaranja.blogspot.com">Amazing Notebooks</a></strong> by ex-engineer José Naranja. 5/5</li>
<li><strong><a href="http://www.bldgblog.com/">BLDGBLOG</a>:</strong> Great blog of the old days. Still kicking. 5/5</li>
<li><strong><a href="https://lospec.com/">LOSPEC</a>:</strong> Cool site with pixel-art resources. Great color-palettes. 4/5</li>
</ul>
<h3 id="youtube">Youtube</h3>
<ul>
<li><strong><a href="https://youtu.be/DmqFbgKWoao">How to draw like Kim Jung Gi</a>:</strong> Super impressive to see a grand master draw. 4/5</li>
<li><strong><a href="https://youtu.be/5CGKdmsPmXY">Taleb Interview</a>:</strong> Nice interview with the probabilist Nassim Taleb about behavioral rules in complex systems. 4/5</li>
<li><strong><a href="https://youtu.be/8z9F0hYPZ7M">These Old People Love Doing Parkour!</a>:</strong> Short videos about old people doing parkour. Very local. 4/5</li>
<li><strong><a href="https://youtu.be/dwI5b-wRLic">The Two Types of Random | Game Maker&rsquo;s Toolkit</a>:</strong> I don&rsquo;t know why but I am always fascinated by game-design. 3/5</li>
<li><strong><a href="https://youtu.be/dveUrpp6vs8">Jenny Odell, How to Do Nothing - XOXO Festival (2019)</a>:</strong> This is a great talk, by a cool artist. 5/5</li>
<li><strong><a href="https://youtu.be/SG3BYLv403k">Mark Suciu: Cross Continental</a>:</strong> A skateboard video like in the good old times - just a bit more technical than I remember them. 3/5</li>
</ul>
<h3 id="play">Play</h3>
<ul>
<li><strong><a href="https://www.katanazero.com/">Katana Zero</a>:</strong> Great game. A bit short. 5/4</li>
</ul>
<hr>
<h2 id="09-september-2018">09-September-2018</h2>
<p>Some of my favorites from summer 2018:</p>
<h3 id="birds">Birds</h3>
<p><a href="https://twitter.com/thespite">Jaume Sanchez Elias</a> posts amazing geometric animations. <a href="https://twitter.com/mtrc">Mike Cook</a> thinks about gaming, procedural content generation and computational creativity (he also wrote a <a href="https://mitpress.mit.edu/books/twitterbots">book about twitter-bots</a>, which I have not read yet, but looks great). <a href="https://twitter.com/kriegundfreitag">Krieg und Freitag</a> is one of the funniest accounts I came across (German). <a href="https://twitter.com/severeweatherEU">Severe-weather.eu</a> collects reports of sever-weather/natural-catastrophes around Europe. <a href="https://twitter.com/GalaxyKate">Kate Compton</a>. <a href="https://twitter.com/gabrielpeyre">Gabriel Peyré</a> provides all kind of info and visualizations about optimal transport.</p>
<h3 id="www">WWW</h3>
<p><a href="http://dataphys.org/list">Physical Visualization</a> is a thing. I especially love <a href="https://www.behance.net/gallery/68572509/LIVING-MAP">this living map</a>, which uses of moss to visualize precipitation patterns over Europe. The <a href="https://medium.com/processing-foundation/hello-p5-js-web-editor-b90b902b74cf">p5.js web-editor</a> looks great. Found an amazing comic that explains the concept of <a href="http://www.stuartmcmillen.com/comic/energy-slaves/">energy-slaves</a>. Tabletop Whale is an amazing blog about science illustration. As an examples see <a href="(http://tabletopwhale.com/2014/09/29/flight-videos-deconstructed.html)">this</a> visualization, which examines the flight-patterns of birds. <a href="https://mikesmathpage.wordpress.com/">Mikes Math Page</a> made a nice <a href="https://mikesmathpage.wordpress.com/2018/06/16/helping-kids-understand-when-the-central-limit-theorem-applies-and-when-it-doesnt/">demonstration</a> about the applicability of the central limit theorem applies.</p>
<p><a href="https://medium.com/@mijordan3">Michael Jordan</a> wrote an <a href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">essay</a> about the emerging of a new kind of engineering from the current developments in ML. <a href="http://www.perell.com/blog/the-algorithmic-trap">The Algorithmic Trap</a> is a thoughtful essay about the arising difficulties to find serendipity while traveling (related: <a href="https://subpixel.space/entries/after-authenticity/">After Authenticity</a>). <a href="https://medium.com/@retronator">Matej ‘Retro’ Jan</a> writes about learning and (pixel) art.  <a href="http://seankross.com/">Sean Kross</a> is a great blog roughly evolving around the <em>&ldquo;data science with R&rdquo;</em>-universe. <a href="https://p5js.org/">P5.js</a> has an native (online) <a href="https://medium.com/processing-foundation/hello-p5-js-web-editor-b90b902b74cf">editor</a> now! Yes, yes, yes! There was an <a href="https://explorabl.es/jam/">explorables-jam</a>, with many, many <a href="https://explorabl.es/">explorable explanations</a>. <a href="https://jlpaca.github.io/toybox/">This</a> website is a collection of small interactive simulations (akin to the explorables from before). <a href="https://idyll.pub/post/dimensionality-reduction-293e465c2a3443e8941b016d/">The Beginner&rsquo;s Guide to Dimensionality Reduction</a> is a nice little intro to the topic of embedding of high dimensional data (also take a look at <a href="https://projector.tensorflow.org/">this tool</a>). <a href="https://medium.com/retronator-magazine/learning-with-video-games-1609ba9d7ea8">Here</a> are some ideas about how one can learn with video games. Finally, <a href="https://towardsdatascience.com/what-a-disentangled-net-we-weave-representation-learning-in-vaes-pt-1-9e5dbc205bd1">here</a> and <a href="https://towardsdatascience.com/with-great-power-comes-poor-latent-codes-representation-learning-in-vaes-pt-2-57403690e92b">here</a> are some very good writings about <a href="https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)">Variational-Autoencoders</a>.</p>
<p>Honorable Mentions: <a href="https://blog.codinghorror.com/our-programs-are-fun-to-use/">Our Programs Are Fun To Use</a>, <a href="http://www.tylermw.com/3d-printing-rayshader/">From R To Reality: 3D Printig Maps With Rayshader</a>, <a href="http://www.tylermw.com/throwing-shade/">Throwing Shade At The Cartographer Illumanity</a>, <a href="https://towardsdatascience.com/@cody.marie.wild">Codie Marie Wild on Medium</a>, <a href="http://conceptviz.github.io/#/e30=">Gallery of Concept Visualization</a>, <a href="https://medium.com/@Elijah_Meeks">Elijah Meeks on Medium</a>, <a href="https://juliagalef.com/2018/08/16/why-we-need-a-new-word-for-lazy/">How to think about lazyness</a>, <a href="http://floooh.github.io/2018/06/02/one-year-of-c.html">One year of C</a>, <a href="http://varianceexplained.org/posts/">Variance Explained</a></p>
<h3 id="moving-pictures">Moving Pictures</h3>
<!-- raw HTML omitted -->
<blockquote>
<p>Nychos is an illustrator, Urban Art- and Graffitiartist , who became known with his street concept RABBIT EYE MOVEMENT (REM) 10 years ago. The icon of the movement is a white rabbit, which has been breeding since then and has been popping up in the streets all over the globe for the past decade. This is exactly what Nychos thrives for – he travels the world to spread his art and his REM concept. Within the last two years Nychos was accompanied by filmmaker Christian Fischer who recorded these journeys to create a full lenght movie. &lsquo;&lsquo;The Deepest Depths Of The Burrow&rsquo;&rsquo; is a documentary about art, lifestyle and subculture. The movie captures the experiences of Nychos and other members of RABBIT EYE MOVEMENT within the last several years. Exceeding the Austrian scene, they flew from Berlin to San Francisco to Copenhagen to Hawaii and many more places to visit artists who all provide a unique view into their (street) culture - They lead Nychos through the streets of their home cities and show them the deepest depths of their burrows. &lsquo;&lsquo;Travel to paint, paint to travel&rsquo;&rsquo; is the motto,which runs like a golden thread throughout the entire movie. Words, which catch the spirit of RABBIT EYE MOVEMENT. &lsquo;&lsquo;The Deepest Depths Of The Burrow&rsquo;&rsquo; shows images which highlight the contrasts between Urban Art and Graffiti from various angles and captures the mutual passion for Urban Art in an unique way.</p>
</blockquote>
<!-- raw HTML omitted -->
<blockquote>
<p>Boiling coffee with a mokka. A movie made with neutron images that shows the coffe making process. The movie was made by A. Kaestner at the cold neutron imaging beam line ICON. The movie is four times faster than in real time.</p>
</blockquote>
<!-- raw HTML omitted -->
<blockquote>
<p>FULLSTERKUR is the third documentary in a collection of films produced by Rogue Fitness, exploring strength culture around the world, connected specifically by the ancient tradition of stone lifting.</p>
</blockquote>
<blockquote>
<p>Nestled at the doorstep of the Arctic Circle, the country of Iceland is uniquely acquainted with the relationship between strength and survival. For hundreds of years, men and women were challenged to overcome harsh weather and endless winter nights by developing their own distinct physical and mental fortitude—passed down from the age of the Vikings, and iconically represented by the lifting of heavy stones. Today, on an island with a population of just over 300,000, a disproportionate number of the world’s greatest strength athletes still call Iceland home.</p>
</blockquote>
<blockquote>
<p>The film features some of the modern stars of Iceland strength, including Magnus Ver Magnuson, Hafthor Bjornsson, and Annie Thorisdottir. But it also sheds light on strength culture’s early roots in the region, from the traditions of the Vikings and Sagas to the lives of farmers and fishermen.</p>
</blockquote>
<!-- raw HTML omitted -->
<blockquote>
<p>Adam Savage checks in on the status of sculptor Johnny Fraser-Allen&rsquo;s tabletop model, which has grown from the intricate Labyrinth we saw two years ago to a sprawling world steeped in fantasy architecture and landscapes that come alive. We&rsquo;re awestruck by the design and details of these models, which Johnny has brilliantly made to be modular and customizable. We can&rsquo;t wait to play in Hagglethorn Hollow!</p>
</blockquote>
<h3 id="sciency-stuff">Sciency Stuff</h3>
<!-- raw HTML omitted -->
<p><a href="http://paradise.caltech.edu/~cook/papers/TwoNeurons.pdf">It Takes Two Neurons To Ride a Bicycle</a> is a beautiful paper by Mathew Cook, which explores a simple ANN model for steering bicycles. Here is the abstract (minus the reference-numbers):</p>
<blockquote>
<p>Past attempts to get computers to ride bicycles have required an inordinate amount of learning time (1700 practice rides for a reinforcement learning approach, while still failing to be able to ride in a straight line), or have required an algebraic analysis of the exact equations of motion for the specific bicycle to be controlled. Mysteriously, humans do not need to do either of these when learning to ride a bicycle. Here we present a two-neuron network1
that can ride a bicycle in a desired direction (for example, towards a desired goal or along a desired
path), which may be chosen or changed at run time. Just as when a person rides a bicycle, the network is very accurate forlong range goals, but in the short run stability issues dominate the behavior.
This happens not by explicit design, but arises as a natural consequence of how the network controls the bicycle</p>
</blockquote>
<!-- raw HTML omitted -->
<p><a href="https://brohrer.github.io/how_decision_trees_work.html">How Decision Trees work</a> is a very, very nice tutorial about decision trees. Easy to follow and thoughtful.</p>
<!-- raw HTML omitted -->
<p>The <a href="https://en.wikipedia.org/wiki/Makridakis_Competitions">M4 competition</a> happened. <a href="https://www.sciencedirect.com/science/article/pii/S0169207018300785">Here</a> is a succinct summary paper of the most important results. Abstract:</p>
<blockquote>
<p>The M4 competition is the continuation of three previous competitions started more than 45 years ago whose purpose was to learn how to improve forecasting accuracy, and how such learning can be applied to advance the theory and practice of forecasting. The purpose of M4 was to replicate the results of the previous ones and extend them into three directions: First significantly increase the number of series, second include Machine Learning (ML) forecasting methods, and third evaluate both point forecasts and prediction intervals. The five major findings of the M4 Competitions are: 1. Out Of the 17 most accurate methods, 12 were “combinations” of mostly statistical approaches. 2. The biggest surprise was a “hybrid” approach that utilized both statistical and ML features. This method’s average sMAPE was close to 10% more accurate than the combination benchmark used to compare the submitted methods. 3. The second most accurate method was a combination of seven statistical methods and one ML one, with the weights for the averaging being calculated by a ML algorithm that was trained to minimize the forecasting. 4. The two most accurate methods also achieved an amazing success in specifying the 95% prediction intervals correctly. 5. The six pure ML methods performed poorly, with none of them being more accurate than the combination benchmark and only one being more accurate than Naïve2. This paper presents some initial results of M4, its major findings and a logical conclusion. Finally, it outlines what the authors consider to be the way forward for the field of forecasting.</p>
</blockquote>
<!-- raw HTML omitted -->
<p><a href="https://arxiv.org/abs/1806.10729">Procedural Level Generation Improves Generality of Deep Reinforcement Learning</a> is a reflection on how generative approaches can be useful to avoid over-fitting. I believe that the ideas are (even) more general than presented are (probably) relevant for optimization in general. Here is the abstract:</p>
<blockquote>
<p>Over the last few years, deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when networks are trained in a fixed environment, such as a single level in a video game, it will usually overfit and fail to generalize to new levels. When RL agents overfit, even slight modifications to the environment can result in poor agent performance. In this paper, we present an approach to prevent overfitting by generating more general agent controllers, through training the agent on a completely new and procedurally generated level each episode. The level generator generate levels whose difficulty slowly increases in response to the observed performance of the agent. Our results show that this approach can learn policies that generalize better to other procedurally generated levels, compared to policies trained on fixed levels.</p>
</blockquote>
<!-- raw HTML omitted -->
<p><a href="https://arxiv.org/abs/1807.03247">An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</a> presents a thoughtful analysis on some specific  behaviors of Convnets and derives a simple solution for these. (It comes along with a short <a href="https://youtu.be/8yFQc6elePA">explanation video</a> and a <a href="https://eng.uber.com/coordconv/">blog-post</a>). Well, here is the abstract:</p>
<blockquote>
<p>Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either perfect translation invariance or varying degrees of translation dependence, as required by the task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10&ndash;100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.</p>
</blockquote>
<p>Honorable mentions: <a href="https://distill.pub/2018/feature-wise-transformations/">Feature-wise transformations</a>, <a href="https://doi.org/10.1002/wat2.1278">On hypothesis testing in hydrology: Why falsification of models is still a really good idea</a>, The <a href="https://worldmodels.github.io/">World Models</a> paper has a extensive <a href="http://blog.otoro.net/2018/06/09/world-models-experiments/">blog-post</a> now.</p>
<hr>
<h2 id="11-mai-2018">11-Mai-2018</h2>
<p><strong>Favourites: Spring 2018</strong></p>
<p>Here are some of my favourites from Spring 2018.</p>
<h3 id="www-1">www</h3>
<p>I build a <a href="https://twitter.com/botski_kaseman">twitter-bot</a> with <a href="https://cheapbotsdonequick.com/"><em>Cheap Bots, Done Quick</em></a>. It tweets proverbs, heuristics, fallacies and biases in the wayne of the seminal work of Tversky and Kahneman, Gerd Gigerenzer &amp; co.</p>
<!-- raw HTML omitted -->
<p><a href="https://twitter.com/ralphammer">Ralph Hammer</a> started an amazing series of articles about learning to draw (and by implication see). Starting with an exposition about <a href="https://medium.com/personal-growth/seeing-vs-reading-29365d4540e2">seeing vs. reading</a>. Now, why he considers &ldquo;writing software&rdquo; as a low-dopamine activity is beyond me. Be it as it may, the highlight throughout the arcitles are the witty and insightful animations, which underpin the writing. Here an example:</p>
<!-- raw HTML omitted -->
<p>I rediscovered <a href="https://www.instapaper.com/">instapaper</a> through reading <a href="https://jaan.io/info-overload/">this</a> excellent article about life-hacks. Read a nice <a href="http://www.interactiongreen.com/chapter-1-power-zero/">essay about the power of less</a>.</p>
<p><a href="http://countercomplex.blogspot.co.at/">Countercomplex</a>, aka Viznut, is blogging (again). Topics include (but are not limited to) demoscene, algorithmic arts, and  digital degrowth.</p>
<p><strong>Honorable mentions</strong>: <a href="https://scholar.google.at/">google scholar</a>, <a href="https://whereisscihub.now.sh/">where is sci-hub now</a>, <a href="http://phdp.github.io/blog.html">Philippe Desjardins-Proulx</a>, <a href="https://archive.org/details/mundussubterrane02kirc">archive.org: Mundus Subterraneus</a>, <a href="https://en.wikipedia.org/wiki/Grammar_induction">wiki: Grammar Induction</a>, <a href="https://magenta.tensorflow.org/music-vae">musicVAE</a>, <a href="http://www.joelsimon.net/corals.html">Evolving Alien Corals</a>, <a href="https://www.nationalgeographic.com/archaeology-and-history/magazine/2017/01-02/babylon-mesopotamia-ancient-city-iraq/">Beautiful Babylon: Jewel of the Ancient World</a>, <a href="https://medium.com/@ben_fry/reconnecting-china-29c3752ab9a0">Reconnecting China</a>, <a href="http://rocs.hu-berlin.de/explorables/">Complexity Explorables</a>.</p>
<h3 id="video">Video</h3>
<p><a href="https://www.youtube.com/user/Nerdwriter1">Nerdwriter</a> single-handedly changed my opinion about youtube and teached me what a video essay is. Special recommendations: <a href="https://youtu.be/9wUePyAqPuk">&ldquo;I Drunkenly Paid for 17 Federal Programs&rdquo;</a> and <a href="https://youtu.be/uBUnmdd5-iA">&ldquo;Mr. Bean Is A Master Of Physical Comedy&rdquo;</a>.</p>
<p>Tadashi Tokieda: <a href="https://youtu.be/D15L0E1zHJ0">Science from a Sheet of Paper</a>. The blurb reads: <em>By curling, folding, crumpling, sometimes tearing paper, Tadashi Tokieda will explore a variety of unexpected phenomena - from geometry and the traditional art of origami, to magic tricks and engineering of materials.</em></p>
<!-- raw HTML omitted -->
<p><a href="http://woodblock.com/">David Bull</a> is a Tokyo-based wooblock printer and probably one of the most wholesome person on earth. His <a href="https://www.youtube.com/user/seseragistudio/videos">youtube-channel</a> is full of information about a subject I know almost nothing about. If nothing else, check out his <a href="https://www.youtube.com/playlist?list=PLK-Wicsj5rAbhRUOqhI85FZAN7df9KlZv"><em>&ldquo;David&rsquo;s Choice&rdquo;</em></a> series. In it he introduces &ldquo;various things that he finds interesting in the world of traditional Japanese woodblock prints&rdquo;.</p>
<p><a href="https://www.youtube.com/channel/UCg3qsVzHeUt5_cPpcRtoaJQ">This guy</a> makes knive out of everything (e.g. <a href="https://youtu.be/MeNR0guNn70">noodles</a>).</p>
<p>The <a href="https://www.youtube.com/channel/UCR0SIzN1XnqQfHMoSXGWxlQ/featured"><em>On Doubt</em></a> interviews are on point. Especially the <a href="https://youtu.be/HtKmLciKO30">mini-documentary about dwarf fortress</a>.</p>
<p><strong>Honorable mentions</strong>: <a href="https://youtu.be/XXRBLyQQ78A">Only Slightly Exagerated: Travel Origon</a>, <a href="https://youtu.be/HqOqNfwQygc">Japanese vs. German Knives</a>, <a href="https://youtu.be/pZtIDeNk2aI">Worlds Smalles Sushi</a>, <a href="https://youtu.be/bGR2qXTPmJc">Omakase Japan</a>, <a href="https://youtu.be/OCmolso_LrQ">Best of the Worst: Suburban Sasquatch</a>, <a href="https://youtu.be/94-k9emqRBY">descriptive mechanics</a>, <a href="https://youtu.be/go5Au01Jrvs">Hadley Wickham: Whole Game</a>, <a href="https://youtu.be/atiYXm7JZv0">Machine Learning with R and Tensorflow</a>.</p>
<h3 id="research">Research</h3>
<p>Was involved in:</p>
<ul>
<li>Karsten Schulz, Reinhard Burgholzer, Daniel Klotz, Johannes Wesemann, and Mathew Herrnegger (2018): <a href="https://www.hydrol-earth-syst-sci.net/22/2607/2018/">Demonstrating the unit hydrograph and flow routing processes involving active student participation – a university lecture experiment</a></li>
<li>Frederik Kratzert, Daniel Klotz, Claire Brenner, Karsten Schulz, Mathew Herrnegger (2018):<a href="https://eartharxiv.org/qv5jz">Rainfall-Runoff modelling using Long-Short-Term-Memory (LSTM) networks</a></li>
</ul>
<p><a href="https://distill.pub/2017/aia/"><em>Using Artiﬁcial Intelligence to Augment Human Intelligence</em> (2017)</a> by <a href="http://shancarter.com/">Shan Carter</a> and <a href="http://michaelnielsen.org/">Michael Nielsen</a> came out in Winter and I read it back then. Recently revisited it and I still love it.</p>
<!-- raw HTML omitted -->
<p><a href="https://twitter.com/decodyng">Cody M. Wild</a> wrote some of the best medium articles about <a href="https://towardsdatascience.com/@cody.marie.wild">machine learning</a> out there (sspecially the series on autoencoders: <a href="https://towardsdatascience.com/what-a-disentangled-net-we-weave-representation-learning-in-vaes-pt-1-9e5dbc205bd1">pt.1</a> and <a href="https://towardsdatascience.com/with-great-power-comes-poor-latent-codes-representation-learning-in-vaes-pt-2-57403690e92b">pt.2</a>).</p>
<p><a href="https://arxiv.org/abs/1803.10122"><em>World Models</em> (2018)</a> by the adorable <a href="https://twitter.com/hardmaru">David Ha</a> and the mighty <a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber</a> came out and it is great. It is not only very interesting topic-wise, but also one of the most well written papers I came across. Heck, it even has it&rsquo;s <a href="https://t.co/diRMWBqpys">own (interactive) website</a>.</p>
<p>Revisited [Vít Klemeš (1983): <em>Conceptualization and scale in hydrology</em>] (<a href="https://www.sciencedirect.com/science/article/pii/0022169483902081)">https://www.sciencedirect.com/science/article/pii/0022169483902081)</a>. The amount of insight/foresight of Klemeš remains awe-inspiring.</p>
<p><a href="http://colingray.me/wp-content/uploads/2018_Grayetal_CHI_DarkPatternsUXDesign.pdf">This</a> is a realy nice read about <a href="https://darkpatterns.org/"><em>dark patterns</em></a> in UX design (If you don&rsquo;t know what dark patterns are, there is also a nice <a href="https://www.youtube.com/watch?v=kxkrdLI6e6M">Nerdwriter video</a> available).</p>
<p>In defense of toy models: <a href="https://hips.seas.harvard.edu/blog/2014/09/02/which-research-results-will-generalize/">1</a>, <a href="http://worrydream.com/SimulationAsAPracticalTool/">2</a>, <a href="http://cognitivemedium.com/tat/index.html">3</a>, and most importantly <a href="https://youtu.be/2v3ANzWkPVI">4</a> (last one is a video).</p>
<p><strong>Honorable mentions</strong>: <a href="https://arxiv.org/abs/1802.08786">Syntax-Directed Variational Autoencoder for Structured Data</a>, <a href="https://arxiv.org/abs/1506.02078">Visualizing and Understanding Recurrent Networks</a>, <a href="https://www.hydrol-earth-syst-sci.net/21/4323/2017/">Toward seamless hydrologic predictions across spatial scales</a>, <a href="https://www.hydrol-earth-syst-sci.net/21/3427/2017/">The evolution of process-based hydrologic models: historical challenges and the collective quest for physical realism</a>, <a href="https://www.hydrol-earth-syst-sci.net/21/3701/2017/">Scaling, similarity, and the fourth paradigm for hydrology</a>.</p>
<hr>
<h2 id="08-april-2018">08-April-2018</h2>
<ul>
<li><a href="https://www.theguardian.com/books/2016/sep/11/the-pigeon-tunnel-john-le-carre-review">John Le Carré - The Pigeon Tunnel</a>. Reading Carrè is always a pleasure. This one is suprisingly self-reflexive.</li>
<li><a href="https://ello.co/mattiasadolfsson">Matthias Adolfosn</a>. Playfull and witty illustration. Can&rsquo;t get enough of them.</li>
<li><a href="https://books.google.at/books/about/Neural_Networks_and_Fuzzy_Systems.html?id=fbJQAAAAMAAJ&amp;redir_esc=y">Kosko - Neural Networks and Fuzzy Systems</a>: An oldie. Intersting to get a grasp of the historical developement of artifical neural networks and fuzzy logic.</li>
<li><a href="https://youtu.be/VrhoOzW8oF8">Song of the Sea</a>. &lt;3</li>
<li><a href="https://youtu.be/m8pGJBgiiDU">Patterson</a>. Don&rsquo;t get fooled by the trailer. The movie is very slow; almost meditative. Watched it three times now, and i like it more with every viewing.</li>
<li><a href="https://www.youtube.com/watch?v=Bb1_zlrjo1c">Youtube: Inverse Problems with TensorFlow</a>. I don&rsquo;t like fusion-power as a technology, but I like thinking about inverse problems. Pretty nice presentation!</li>
<li><a href="https://youtu.be/hbzGO_Qonu0">Youtube: How to keep players engaged (without being evil)</a>. &ldquo;<em>Why do some games keep us rapt and entertained until the closing credits, while others fizzle out and end up on our pile of shame? Let’s look at some key ways developers can keep players interested (without being evil about it)</em>&rdquo;</li>
<li><a href="https://www.youtube.com/playlist?list=PLL4iOQ3Gw7L9eyCJ8EnkZgO0p1AzdrRqN">Youtube: 1LIVE Chilly Gonzales - Pop Music Masterclass</a>. Chilly Gonzales gives some intuitions about how pop-music works.</li>
<li><a href="https://youtu.be/owseTngZFMI">Youtube: Forging a knive from alufoil</a>.</li>
<li><a href="https://www.youtube.com/user/Nerdwriter1">Youtube: Nerdwriter</a>. The Renaissance Man of youtube.</li>
<li><a href="https://youtu.be/pbRJtP8_AeE">Youtube: New Media Linguistics</a>. Notes on language creation/invention, by David Peterson (Inventor of the Dorthraki language from GoT)</li>
</ul>

</main>

  <footer>
  <center>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  <a href="https://danklotz.github.io/"><img src="../project-fig.png" alt="hello"></a>
  
  </center>
  </footer>
  </body>
</html>

