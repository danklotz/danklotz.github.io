<!DOCTYPE html>


<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>multitudes | daniel klotz |</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
  </head>

<body>
<center>
  <img src="/danklotz-signature.png", style="height:230px">
  <p style="margin-bottom:-35px;"></p>
  <ul class="menu", margin=0>·
    
    <a href="/">hi</a> ·
    
    <a href="/note/">notes</a> ·
    
    <a href="/gfx/">gfx</a> ·
    
    <a href="/about/">me</a> ·
    
  </ul>
  <br>
 </center>
<p style="margin-bottom:-50px;"></p>
</body>

<div class="article-meta">

<h1><span class="title">multitudes</span></h1>


</div>

<main>
<p>Not sure where this goes. For now its a loose collection. More a collage than anything.</p>
<h1 id="religion">Religion</h1>
<p><a href="https://en.wikipedia.org/wiki/Indra%27s_net">Idra&rsquo;s net</a></p>
<h1 id="creatures">Creatures</h1>
<ul>
<li><a href="https://villains.fandom.com/wiki/Ygramul">Ygramul, The Many</a> &hellip; literally swarm intelligence.</li>
</ul>
<h1 id="many-dimensions">Many dimensions</h1>
<p><a href="https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking">Intuitions for higher dimensional thinking</a></p>
<h1 id="many-models">Many models</h1>
<ol start="2">
<li>Cheung et al. (2019): <a href="https://arxiv.org/abs/1902.05522">Superposition of many models into one</a>. Mixing different model initialization to create a model that can adapt to different task. Below the first figure and the absract go get an idea.</li>
</ol>
<p><img src="/note/2020-12-14-multi-multi_files/superposition.png" alt=""></p>
<blockquote>
<p><em>Abstract</em>: We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.</p>
</blockquote>
<h1 id="many-objectives">Many objectives</h1>
<p>Google AI (2020): <a href="https://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html?m=1">Optimizing Multiple Loss Functions with Loss-Conditional Training</a> (<a href="https://openreview.net/pdf?id=HyxY6JHKwr">paper</a>)</p>
<p>![](/note/2020-12-14-multi-multi_files/Screenshot from 2020-12-14 01-15-44.png)</p>
<blockquote>
<p><em>First paragraph:</em> In many machine learning applications the performance of a model cannot be summarized by a single number, but instead relies on several qualities, some of which may even be mutually exclusive. For example, a learned image compression model should minimize the compressed image size while maximizing its quality. It is often not possible to simultaneously optimize all the values of interest, either because they are fundamentally in conflict, like the image quality and the compression ratio in the example above, or simply due to the limited model capacity. Hence, in practice one has to decide how to balance the values of interest.</p>
</blockquote>

</main>

  <footer>
  <center>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  <a href="https://danklotz.github.io/"><img src="../project-fig.png" alt="hello"></a>
  
  </center>
  </footer>
  </body>
</html>

